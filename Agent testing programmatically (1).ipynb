{
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "lastEditStatus": {
   "notebookId": "7qvpohc2s2ipit6oa567",
   "authorId": "48480042761",
   "authorName": "DARIA",
   "authorEmail": "daria.rostovtseva@snowflake.com",
   "sessionId": "e34c24dc-39ce-491d-9c7b-1d89b04b8142",
   "lastEditTime": 1765558682189
  }
 },
 "nbformat_minor": 2,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000000",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "# Snowflake Agent Testing at Scale\n\nThis notebook tests your agent by looping through sample questions and capturing responses using the Snowflake REST API.\n\n**In this sample notebook, the agent details are as follows:**\n- Agent Name: SERVICENOW_ANALYTICS_AGENT\n- Database: SNOWFLAKE_INTELLIGENCE\n- Schema: AGENTS\n\n**Note:** Agents must be called via the REST API endpoint: `/api/v2/databases/{DB}/schemas/{SCHEMA}/agents/{AGENT}:run`\n"
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000001",
   "metadata": {
    "name": "cell2"
   },
   "source": [
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000002",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": "# Import required libraries\nimport _snowflake\nimport snowflake.snowpark as snowpark\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark.context import get_active_session\nimport pandas as pd\nimport json\nfrom datetime import datetime\n\n# Get the current session (automatically available in Snowflake notebooks)\nsession = get_active_session()\n\n# Agent configuration\nAGENT_DATABASE = \"SNOWFLAKE_INTELLIGENCE\"\nAGENT_SCHEMA = \"AGENTS\"\nAGENT_NAME = \"SERVICENOW_ANALYTICS_AGENT\" #update with your agent name\n\nprint(f\"✓ Session established\")\nprint(f\"Agent: {AGENT_DATABASE}.{AGENT_SCHEMA}.{AGENT_NAME}\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000003",
   "metadata": {
    "name": "cell4"
   },
   "source": [
    "## Define Sample Questions\n",
    "\n",
    "Create a list of test questions to send to the agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000004",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": [
    "# Sample questions for testing\n",
    "sample_questions = [\n",
    "    \"What are the top 5 incident categories by count?\",\n",
    "    \"Show me the average resolution time for high priority incidents\",\n",
    "    \"How many incidents were opened last month?\",\n",
    "    \"What is the current backlog of unresolved incidents?\",\n",
    "    \"Which teams have the highest incident volume?\"\n",
    "]\n",
    "\n",
    "print(f\"Total test questions: {len(sample_questions)}\")\n",
    "for i, q in enumerate(sample_questions, 1):\n",
    "    print(f\"{i}. {q}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000005",
   "metadata": {
    "name": "cell6"
   },
   "source": [
    "## Call Agent Using REST API\n",
    "\n",
    "Snowflake agents are called via the REST API using `_snowflake.send_snow_api_request()`.\n",
    "The endpoint format is: `/api/v2/databases/{DB}/schemas/{SCHEMA}/agents/{AGENT}:run`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000006",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "# Function to send a message to the agent\n",
    "def send_agent_message(prompt: str) -> dict:\n",
    "    \"\"\"Calls the Agent REST API and returns the response.\"\"\"\n",
    "    \n",
    "    # Construct the API endpoint\n",
    "    endpoint = f\"/api/v2/databases/{AGENT_DATABASE}/schemas/{AGENT_SCHEMA}/agents/{AGENT_NAME}:run\"\n",
    "    \n",
    "    # Prepare the request body\n",
    "    request_body = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Make the API call\n",
    "    resp = _snowflake.send_snow_api_request(\n",
    "        \"POST\",\n",
    "        endpoint,\n",
    "        {},      # path_params\n",
    "        {},      # query_params\n",
    "        request_body,\n",
    "        {},      # headers\n",
    "        30000    # timeout in ms\n",
    "    )\n",
    "    \n",
    "    if resp[\"status\"] < 400:\n",
    "        return json.loads(resp[\"content\"])\n",
    "    else:\n",
    "        raise Exception(\n",
    "            f\"Failed request with status {resp['status']}: {resp}\"\n",
    "        )\n",
    "\n",
    "# Test the agent with a sample question\n",
    "test_question = sample_questions[0]\n",
    "\n",
    "try:\n",
    "    print(f\"Testing agent with question: {test_question}\\n\")\n",
    "    response = send_agent_message(test_question)\n",
    "    \n",
    "    print(\"✓ Successfully called agent via REST API\")\n",
    "    print(f\"\\nResponse structure:\")\n",
    "    print(json.dumps(response, indent=2))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error calling agent: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000007",
   "metadata": {
    "name": "cell8"
   },
   "source": [
    "## Understanding Agent Response Format\n",
    "\n",
    "The agent returns a structured response with message content that may include text, SQL, suggestions, and more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000008",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": [
    "# Function to extract text response from agent streaming events\n",
    "def extract_agent_response(response_events: list) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the text response from the agent API streaming response.\n",
    "    \n",
    "    The agent returns an array of streaming events. The final consolidated\n",
    "    response is in the event where event == 'response'.\n",
    "    \n",
    "    Args:\n",
    "        response_events: List of streaming event dictionaries from the agent\n",
    "        \n",
    "    Returns:\n",
    "        Formatted string with the agent's response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Response events is a list - find the final 'response' event\n",
    "        final_response = None\n",
    "        for event in response_events:\n",
    "            if event.get('event') == 'response':\n",
    "                final_response = event.get('data', {})\n",
    "                break\n",
    "        \n",
    "        if not final_response:\n",
    "            return f\"No final response found in events. Got {len(response_events)} events.\"\n",
    "        \n",
    "        # Extract content from the final response\n",
    "        content = final_response.get(\"content\", [])\n",
    "        response_parts = []\n",
    "        \n",
    "        for item in content:\n",
    "            item_type = item.get(\"type\")\n",
    "            \n",
    "            if item_type == \"text\":\n",
    "                response_parts.append(item.get(\"text\", \"\"))\n",
    "                \n",
    "            elif item_type == \"thinking\":\n",
    "                thinking_text = item.get(\"thinking\", {}).get(\"text\", \"\")\n",
    "                if thinking_text:\n",
    "                    response_parts.append(f\"[Thinking: {thinking_text}]\")\n",
    "                    \n",
    "            elif item_type == \"tool_use\":\n",
    "                tool_name = item.get(\"tool_use\", {}).get(\"name\", \"unknown\")\n",
    "                tool_type = item.get(\"tool_use\", {}).get(\"type\", \"unknown\")\n",
    "                response_parts.append(f\"[Used tool: {tool_name} ({tool_type})]\")\n",
    "                \n",
    "            elif item_type == \"tool_result\":\n",
    "                # Extract SQL and data from tool results\n",
    "                tool_result = item.get(\"tool_result\", {})\n",
    "                tool_content = tool_result.get(\"content\", [])\n",
    "                \n",
    "                for tc in tool_content:\n",
    "                    if tc.get(\"type\") == \"json\":\n",
    "                        json_data = tc.get(\"json\", {})\n",
    "                        if \"sql\" in json_data:\n",
    "                            response_parts.append(f\"\\n[SQL Query]\\n{json_data['sql']}\")\n",
    "                        if \"text\" in json_data:\n",
    "                            response_parts.append(json_data[\"text\"])\n",
    "                            \n",
    "            elif item_type == \"chart\":\n",
    "                response_parts.append(\"[Chart generated - see visualization]\")\n",
    "        \n",
    "        return \"\\n\\n\".join(response_parts) if response_parts else \"No response content found\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error parsing response: {str(e)}\\nResponse type: {type(response_events)}\"\n",
    "\n",
    "# Test extraction with the previous response\n",
    "test_question = sample_questions[0]\n",
    "\n",
    "try:\n",
    "    response = send_agent_message(test_question)\n",
    "    extracted = extract_agent_response(response)\n",
    "    \n",
    "    print(f\"Question: {test_question}\\n\")\n",
    "    print(f\"Extracted Response:\")\n",
    "    print(\"=\"*80)\n",
    "    print(extracted)\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Also show number of events received\n",
    "    print(f\"\\nReceived {len(response)} streaming events\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000009",
   "metadata": {
    "name": "cell10"
   },
   "source": [
    "## Quick Test: Final Result Only\n",
    "\n",
    "Test the agent and see only the final clean result without verbose logging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000010",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    "# Quick test - show only the final result\n",
    "test_question = sample_questions[0]\n",
    "\n",
    "print(f\"Question: {test_question}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Call the agent\n",
    "    response = send_agent_message(test_question)\n",
    "    \n",
    "    # Extract and display only the final text answer\n",
    "    extracted = extract_agent_response(response)\n",
    "    print(extracted)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000011",
   "metadata": {
    "name": "cell12"
   },
   "source": [
    "## Understanding Streaming Response Events\n",
    "\n",
    "The agent API returns an array of streaming events that show the agent's progress:\n",
    "- `response.thinking.delta` - Agent's reasoning process (streamed)\n",
    "- `response.tool_use` - Tools the agent is using\n",
    "- `response.tool_result` - Results from tool execution\n",
    "- `response.text.delta` - Response text (streamed)\n",
    "- `response.chart` - Chart specifications (if generated)\n",
    "- `response` - Final consolidated response\n",
    "- `done` - End of stream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000012",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": [
    "# Helper function to analyze streaming events\n",
    "def analyze_streaming_events(events: list) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze the streaming events to understand agent behavior.\n",
    "    \n",
    "    Args:\n",
    "        events: List of event dictionaries from agent response\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with event statistics and extracted information\n",
    "    \"\"\"\n",
    "    event_types = {}\n",
    "    tools_used = []\n",
    "    text_parts = []\n",
    "    sql_queries = []\n",
    "    charts = []\n",
    "    \n",
    "    for event in events:\n",
    "        event_type = event.get('event', 'unknown')\n",
    "        event_types[event_type] = event_types.get(event_type, 0) + 1\n",
    "        \n",
    "        data = event.get('data', {})\n",
    "        \n",
    "        # Track tools used\n",
    "        if event_type == 'response.tool_use':\n",
    "            tool_name = data.get('name', 'unknown')\n",
    "            tool_type = data.get('type', 'unknown')\n",
    "            tools_used.append(f\"{tool_name} ({tool_type})\")\n",
    "        \n",
    "        # Extract text deltas\n",
    "        if event_type == 'response.text.delta':\n",
    "            text_parts.append(data.get('text', ''))\n",
    "        \n",
    "        # Extract SQL from tool results\n",
    "        if event_type == 'response.tool_result':\n",
    "            content = data.get('content', [])\n",
    "            for item in content:\n",
    "                if item.get('type') == 'json':\n",
    "                    json_data = item.get('json', {})\n",
    "                    if 'sql' in json_data:\n",
    "                        sql_queries.append(json_data['sql'])\n",
    "        \n",
    "        # Track charts\n",
    "        if event_type == 'response.chart':\n",
    "            charts.append(data.get('chart_spec', ''))\n",
    "    \n",
    "    return {\n",
    "        'total_events': len(events),\n",
    "        'event_types': event_types,\n",
    "        'tools_used': tools_used,\n",
    "        'full_text': ''.join(text_parts),\n",
    "        'sql_queries': sql_queries,\n",
    "        'charts_generated': len(charts)\n",
    "    }\n",
    "\n",
    "# Analyze a test response\n",
    "test_question = sample_questions[0]\n",
    "\n",
    "try:\n",
    "    print(f\"Question: {test_question}\\n\")\n",
    "    response = send_agent_message(test_question)\n",
    "    \n",
    "    # Analyze the streaming events\n",
    "    analysis = analyze_streaming_events(response)\n",
    "    \n",
    "    print(\"Event Analysis:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total Events: {analysis['total_events']}\")\n",
    "    print(f\"\\nEvent Type Counts:\")\n",
    "    for event_type, count in sorted(analysis['event_types'].items()):\n",
    "        print(f\"  {event_type}: {count}\")\n",
    "    \n",
    "    print(f\"\\nTools Used: {', '.join(analysis['tools_used']) if analysis['tools_used'] else 'None'}\")\n",
    "    print(f\"SQL Queries Generated: {len(analysis['sql_queries'])}\")\n",
    "    print(f\"Charts Generated: {analysis['charts_generated']}\")\n",
    "    \n",
    "    if analysis['full_text']:\n",
    "        print(f\"\\nFull Response Text:\")\n",
    "        print(\"-\"*80)\n",
    "        print(analysis['full_text'])\n",
    "    \n",
    "    if analysis['sql_queries']:\n",
    "        print(f\"\\nSQL Queries:\")\n",
    "        print(\"-\"*80)\n",
    "        for i, sql in enumerate(analysis['sql_queries'], 1):\n",
    "            print(f\"\\nQuery {i}:\")\n",
    "            print(sql[:500] + \"...\" if len(sql) > 500 else sql)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000013",
   "metadata": {
    "name": "cell14"
   },
   "source": [
    "## Batch Testing: Loop Through All Questions\n",
    "\n",
    "Now let's loop through all sample questions and capture responses with full metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000014",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": [
    "# Function to call agent and capture response with full metadata\n",
    "def call_agent(question):\n",
    "    \"\"\"\n",
    "    Call the agent with a question via REST API and return the response.\n",
    "    \n",
    "    Args:\n",
    "        question: The question to ask the agent\n",
    "    \n",
    "    Returns:\n",
    "        dict with question, response, raw response, and metadata\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # Call the agent via REST API\n",
    "        raw_response = send_agent_message(question)\n",
    "        \n",
    "        # Extract readable response\n",
    "        extracted_response = extract_agent_response(raw_response)\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"response\": extracted_response,\n",
    "            \"raw_response_json\": json.dumps(raw_response, default=str),  # Convert to JSON string\n",
    "            \"num_events\": len(raw_response) if isinstance(raw_response, list) else 0,\n",
    "            \"status\": \"success\",\n",
    "            \"duration_seconds\": duration,\n",
    "            \"timestamp\": start_time.isoformat(),\n",
    "            \"error\": None\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"response\": None,\n",
    "            \"raw_response_json\": None,\n",
    "            \"num_events\": 0,\n",
    "            \"status\": \"error\",\n",
    "            \"duration_seconds\": duration,\n",
    "            \"timestamp\": start_time.isoformat(),\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "print(\"✓ Agent calling function defined successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000015",
   "metadata": {
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": [
    "# Run batch testing\n",
    "print(f\"Starting batch test with {len(sample_questions)} questions...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, question in enumerate(sample_questions, 1):\n",
    "    print(f\"\\n[{i}/{len(sample_questions)}] Testing question: {question}\")\n",
    "    \n",
    "    # Call the agent\n",
    "    result = call_agent(question)\n",
    "    results.append(result)\n",
    "    \n",
    "    if result[\"status\"] == \"success\":\n",
    "        print(f\"✓ Success ({result['duration_seconds']:.2f}s)\")\n",
    "        response_str = str(result['response'])\n",
    "        preview = response_str[:200] + \"...\" if len(response_str) > 200 else response_str\n",
    "        print(f\"Response preview: {preview}\")\n",
    "    else:\n",
    "        print(f\"✗ Error: {result['error']}\")\n",
    "    \n",
    "    print(\"-\"*80)\n",
    "\n",
    "print(f\"\\n\\nBatch testing complete!\")\n",
    "print(f\"Total questions: {len(results)}\")\n",
    "print(f\"Successful: {sum(1 for r in results if r['status'] == 'success')}\")\n",
    "print(f\"Failed: {sum(1 for r in results if r['status'] == 'error')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000016",
   "metadata": {
    "name": "cell17"
   },
   "source": [
    "## Analyze Results\n",
    "\n",
    "Convert results to a DataFrame for easier analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000017",
   "metadata": {
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": [
    "# Convert results to pandas DataFrame for analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"Results Summary:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display key columns (exclude raw_response_json as it's very large)\n",
    "display_cols = ['question', 'status', 'duration_seconds', 'num_events']\n",
    "print(results_df[display_cols].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Note: Full raw responses stored in 'raw_response_json' field\")\n",
    "print(f\"      Use json.loads(results_df.iloc[i]['raw_response_json']) to access\")\n",
    "\n",
    "# Display DataFrame without the JSON column for readability\n",
    "results_df[['question', 'response', 'status', 'duration_seconds', 'num_events', 'timestamp', 'error']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell22"
   },
   "source": [
    "### Accessing Raw Response Data\n",
    "\n",
    "If you need to access the full raw streaming events for a specific result:\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000018"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell23",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Example: Access raw response for the first successful result\n",
    "if len(results_df) > 0 and results_df.iloc[0]['raw_response_json']:\n",
    "    # Parse the JSON string back to Python object\n",
    "    first_raw_response = json.loads(results_df.iloc[0]['raw_response_json'])\n",
    "    \n",
    "    print(f\"First result has {len(first_raw_response)} streaming events\")\n",
    "    print(f\"\\nEvent types in first result:\")\n",
    "    \n",
    "    event_types = {}\n",
    "    for event in first_raw_response:\n",
    "        event_type = event.get('event', 'unknown')\n",
    "        event_types[event_type] = event_types.get(event_type, 0) + 1\n",
    "    \n",
    "    for event_type, count in sorted(event_types.items()):\n",
    "        print(f\"  {event_type}: {count}\")\n",
    "    \n",
    "    # Uncomment to see full raw response structure\n",
    "    # print(\"\\nFull raw response:\")\n",
    "    # print(json.dumps(first_raw_response, indent=2, default=str)[:1000] + \"...\")\n",
    "else:\n",
    "    print(\"No results available yet\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000019"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000020",
   "metadata": {
    "language": "python",
    "name": "cell19"
   },
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "successful_results = results_df[results_df['status'] == 'success']\n",
    "\n",
    "if len(successful_results) > 0:\n",
    "    print(\"Performance Statistics:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total Queries: {len(results_df)}\")\n",
    "    print(f\"Successful: {len(successful_results)}\")\n",
    "    print(f\"Failed: {len(results_df) - len(successful_results)}\")\n",
    "    print(f\"Success Rate: {len(successful_results)/len(results_df)*100:.1f}%\")\n",
    "    print(f\"\\nResponse Time Statistics:\")\n",
    "    print(f\"Average: {successful_results['duration_seconds'].mean():.2f}s\")\n",
    "    print(f\"Median: {successful_results['duration_seconds'].median():.2f}s\")\n",
    "    print(f\"Min: {successful_results['duration_seconds'].min():.2f}s\")\n",
    "    print(f\"Max: {successful_results['duration_seconds'].max():.2f}s\")\n",
    "else:\n",
    "    print(\"No successful results to analyze.\")\n",
    "    print(\"\\nPlease check the error messages above and adjust the agent calling method.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000021",
   "metadata": {
    "name": "cell20"
   },
   "source": [
    "## Save Results to Snowflake Table\n",
    "\n",
    "Optionally save the results to a Snowflake table for later analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000022",
   "metadata": {
    "language": "python",
    "name": "cell21"
   },
   "outputs": [],
   "source": "# Save results to a Snowflake table\n# Uncomment and modify the database/schema as needed\n\n#target_table = \"YOUR_DB.YOUR_SCHEMA.AGENT_TEST_RESULTS\"\n\n#try:\n   # Convert results to Snowpark DataFrame\n    #results_snowpark_df = session.create_dataframe(results)\n    \n   # Write to table\n    #results_snowpark_df.write.mode(\"append\").save_as_table(target_table)\n     \n    #print(f\"✓ Results saved to {target_table}\")\n#except Exception as e:\n    #print(f\"✗ Error saving results: {str(e)}\")\n\nprint(\"To save results, uncomment the code above and set your target table name.\")\n"
  }
 ]
}